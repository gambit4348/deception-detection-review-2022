{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deception Detection supported by Machine Learning\n",
    "## Literature review - Data collection and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Instantiating a BiblioAlly Catalog in a SQLite file **DeceptionDetection.db**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BiblioAlly as ally\n",
    "\n",
    "base_path = \"./\"\n",
    "base_file = base_path + \"DeceptionDetection.db\"\n",
    "base = ally.Catalog(base_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Running queries on scientific online search engines\n",
    "\n",
    "This process is not yet supported by BiblioAlly. At this moment we have to use many search engines and run the queries ourselves.\n",
    "\n",
    "**Reminder**: Many search engines provide an API that could be used to automate this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Export results as BibTeX files\n",
    "This process is not yet supported by BiblioAlly and is only needed because BiblioAlly still can't run queries directly.\n",
    "So, we have to collect the results of our queries and save them as BibTex files that will further be imported into the BiblioAlly Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Import all BibTeX files into BiblioAlly\n",
    "Importing BibTex files from Web of Science, Scopus, ACM Digital Library and IEEEXplore after they were exported from their original search engine pages.\n",
    "Here we start to load data into BiblioAlly.\n",
    "\n",
    "All imported documents are tagged as **IMPORTED**. BiblioAlly will try to spot duplicates and tagged them as **DUPLICATE**. The duplicates are still imported, but\n",
    "they are appropriately marked and a reference to the existing document is set so this can be feed some statistics.\n",
    "\n",
    "BiblioAlly strips all non-alphanumeric characters from the article's title, convert it to lowercase and calculates a CRC32. That CRC32 is used to detect the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BiblioAlly.acmdl as acm\n",
    "import BiblioAlly.ieee as ieee\n",
    "import BiblioAlly.scopus as scopus\n",
    "import BiblioAlly.wos as wos\n",
    "\n",
    "refs_path = base_path + 'Refs\\\\'\n",
    "loadCount, fileCount, baseCount = base.import_from_file(wos.WebOfScience, refs_path + 'WoS\\\\refs.bib')\n",
    "print(f\"Web os Science: File={fileCount} Load={loadCount} Base={baseCount}\")\n",
    "loadCount, fileCount, baseCount = base.import_from_file(scopus.Scopus, refs_path + 'Scopus\\\\refs.bib')\n",
    "print(f\"Scopus        : File={fileCount} Load={loadCount} Base={baseCount}\")\n",
    "loadCount, fileCount, baseCount = base.import_from_file(acm.AcmDL, refs_path + 'AcmDL\\\\refs.bib')\n",
    "print(f\"ACM Dig Lib   : File={fileCount} Load={loadCount} Base={baseCount}\")\n",
    "loadCount, fileCount, baseCount = base.import_from_file(ieee.IeeeXplore, refs_path + 'IeeeXplore\\\\refs.bib')\n",
    "print(f\"IEEE Xplore   : File={fileCount} Load={loadCount} Base={baseCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Manual duplicates detection\n",
    "List all documents not marked as duplicate, sort them by title and manually inspect the list to spot remaining duplicates.\n",
    "After that, we decide which one is the duplicate, then manually mark the duplicates as such and\n",
    "update the base. Unfortunately, BiblioAlly still misses some duplicates.\n",
    "\n",
    "**Reminder**: improve duplicate finder to avoid this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pattern = re.compile('[\\W_]+')\n",
    "def code_title(title):\n",
    "    return pattern.sub('', title).lower()\n",
    "\n",
    "def only_duplicates(documents):\n",
    "    doc_dict = dict()\n",
    "    for doc in documents:\n",
    "        if doc.code_title not in doc_dict:\n",
    "            doc_dict[doc.code_title] = [doc]\n",
    "        else:\n",
    "            doc_dict[doc.code_title].append(doc)\n",
    "    only_dups = []\n",
    "    for docs in doc_dict.values():\n",
    "        if len(docs)>1:\n",
    "            only_dups += docs\n",
    "    return only_dups\n",
    "\n",
    "non_duplicates = base.documents_by(tagged_as=domain.TAG_IMPORTED, untagged_as=domain.TAG_DUPLICATE)\n",
    "for doc in non_duplicates:\n",
    "    doc.code_title = code_title(doc.title)\n",
    "    \n",
    "duplicates = only_duplicates(non_duplicates)\n",
    "duplicates.sort(key=lambda document: document.code_title)\n",
    "\n",
    "dups_dict = [{'id': d.id, 'key': d.external_key, 'year': d.year, 'title': d.title, 'authors': [a.author.long_name for a in d.authors]} for d in duplicates]\n",
    "dups_df = pd.DataFrame(dups_dict)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "dups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we decided which is duplicate and which is not, we select the duplicates to mark them as such and\n",
    "# reference the original documents.\n",
    "doc_ori_dups = [('742412', 'VanDerWalt201641'), ('10.5555/2615731.261748', 'ISI:00037199040001'), ('vanDitmarsch202046', 'ISI:00051732150000'),\n",
    "                ('ISI:00055057660002', 'Vogel201928'), ('ISI:00052556300000', 'Tortora202'), ('ISI:00055522520000', 'SÃ¡nchez-Monedero202')]\n",
    "\n",
    "for key_ori, key_dup in doc_ori_dups:\n",
    "    doc_ori = base.document_by(external_key=key_ori)\n",
    "    doc_dup = base.document_by(external_key=key_dup)\n",
    "    base.tag(doc_dup, domain.TAG_DUPLICATE)\n",
    "    doc_dup.original_document = doc_ori\n",
    "base.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Shallow screening\n",
    "There is a simple user interface for this, so each **IMPORTED** document is shown so it can be evaluated based on \"Title\", \"Keywords\" and \"Abstract\".\n",
    "If a reference does not fit the research goals, it is marked as **EXCLUDED** and an exclusion reason is assigned to it so we can have some further\n",
    "statistics. On the other hand the paper is untagged **IMPORTED** and tagged as **PRE-ACCEPTED**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BiblioAlly.gui as gui\n",
    "\n",
    "browser = gui.Browser(base)\n",
    "browser.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Download PRE-ACCEPTED documents\n",
    "Since BiblioAlly still cannot automatically download the documents, we go for them manually. We have to go for each of the accepted documents to read\n",
    "them entirely. We do this manually since at this moment BiblioAlly is unable to use any APIs to retrieve the documents automatically. So, we\n",
    "list all the pre-accepted documents and manually run some queries in IEEE Xplore, ACM Digital Library or Scholar Google to find and download the\n",
    "corresponding PDF files.\n",
    "\n",
    "All downloaded documents are renamed to the following pattern: `YEAR[AUTHOR]TITLE`, where:\n",
    "1. **YEAR**: publication year, so documents can be sorted by year in the file system;\n",
    "2. **AUTHOR**: the surname of the first author;\n",
    "3. **TITLE**: title of the document.\n",
    "\n",
    "Let's list them in chuncks of 10 and get them by DOI or title, what works first.\n",
    "\n",
    "**NOTE**: Because CAPES (Brazilian Scientific Research Regulator/Sponsor) enable us to have access to IEEE Xplore and ACM Digital Library content,\n",
    "in general we can download even those documents that are not published as Open Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_accepted_docs = base.documents_by(tagged_as=ally.TAG_PRE_SELECTED)\n",
    "pre_accepted_docs.sort(key=lambda doc: [doc.year, doc.title])\n",
    "\n",
    "print(f'{len(pre_accepted_docs)} documents to download and read:\\n')\n",
    "start = 1\n",
    "for index in range(len(pre_accepted_docs)):\n",
    "    doc = pre_accepted_docs[index]\n",
    "    print(f'{index+1:3} [{doc.id:3}] {doc.year}: {doc.generator:20} {doc.title} ({doc.doi})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Deep screening\n",
    "Now we read all the `PRE-ACCEPTED` documents in full, decide either to keep or not each one and, for those ones that meet the research goals,\n",
    "a number of meta-data is extracted and a FreeMind mind map is built as a visual summary of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BiblioAlly.gui as gui\n",
    "\n",
    "browser = gui.Browser(base)\n",
    "browser.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Extract relevant data from selected papers\n",
    "At this moment BiblioAlly does not have any features that make this step easier. We basically have to read the documents, build the\n",
    "mind maps and extract the relevant meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Summarize selected papers\n",
    "Here we summarize all the readings so we can have some statistics for the review. \n",
    "\n",
    "#### Details on this step are located in the 3-Meta-analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document references from IeeeXplore and Scopus do not have author's full name, so we will correct that one-by-one based on\n",
    "# the names directly collected from the PDF files. The code below updates what is needed.\n",
    "\n",
    "def update_long_names(document_id, names, correct_short_name=False):\n",
    "    print(f'Document ID => {document_id}')\n",
    "    document = base.document_by({'id': document_id}, return_first=True)\n",
    "    if document is None:\n",
    "        print('Document not found! ABORTING')\n",
    "        return\n",
    "    for author in document.authors:\n",
    "        if correct_short_name:\n",
    "            parts = author.longName.split()\n",
    "            new_short_name = parts[-1]+', '+' '.join([p[0]+'.' for p in parts[0:len(parts)-1]])\n",
    "        author.longName = names[author.shortName]\n",
    "        if correct_short_name:\n",
    "            author.shortName = new_short_name\n",
    "        base.save_to_db(author)\n",
    "\n",
    "def adjust_long_names(document_id):\n",
    "    print(f'Document ID => {document_id}')\n",
    "    document = base.document_by({'id': document_id}, return_first=True)\n",
    "    if document is None:\n",
    "        print('Document not found! ABORTING')\n",
    "        return\n",
    "    for author in document.authors:\n",
    "        if author.longName is None:\n",
    "            print(f'{author.shortName} has no long name! ABORTING')\n",
    "            continue\n",
    "        if ',' not in author.longName:\n",
    "            continue\n",
    "        parts = author.longName.split(',')\n",
    "        new_long_name = parts[-1].strip()+' '+' '.join([p.strip() for p in parts[0:len(parts)-1]])\n",
    "        author.longName = new_long_name\n",
    "        base.save_to_db(author)\n",
    "\n",
    "print('AJUSTING LONG NAMES')\n",
    "document_ids = [13,19,24,26,28,31,33,37,40,53,61,76,81,107,116,119,130,149,172,174,178,179,180,181,184,189,194,196,199,204,208,\n",
    "214,218,221,224,234,245,250,384,385,388,389,394,395,396,398,401,402,403,410,415,422,423,426,429,432,433,434,436,\n",
    "439,448,451,458,463,467,479,483,493,501,504,527,546\n",
    "]\n",
    "for document_id in document_ids:\n",
    "    adjust_long_names(document_id)\n",
    "\n",
    "    \n",
    "print('\\nCORRECTING LONG NAMES')\n",
    "names = {\n",
    "    'Crockett, K.': 'Keeley Crockett',\n",
    "    'OShea, J.': 'James OâShea',\n",
    "    'Khan, W.': 'Wasiq Khan',\n",
    "}\n",
    "update_long_names(53, names)\n",
    "\n",
    "names = {\n",
    "    'Mizanur R. M.': 'Md. Mizanur Rahman',\n",
    "    'Shome, A.': 'Atanu Shome',\n",
    "    'Chellappan, S.': 'Sriram Chellappan',\n",
    "    'Alim A. I. A.': 'A. B. M. Alim Al Islam',\n",
    "}\n",
    "update_long_names(76, names)\n",
    "\n",
    "names = {\n",
    "    'Fu, H.': 'Hongliang Fu',\n",
    "    'Lei, P.': 'Peizhi Lei',\n",
    "    'Tao, H.': 'Huawei Tao',\n",
    "    'Zhao, L.': 'Li Zhao',\n",
    "    'Yang, J.': 'Jing Yang',\n",
    "}\n",
    "update_long_names(81, names)\n",
    "\n",
    "names = {\n",
    "    'Warnita, T.': 'Tiffani Warnita',\n",
    "    'Lestari, D.': 'Dessi Puji Lestari',\n",
    "}\n",
    "update_long_names(107, names)\n",
    "\n",
    "names = {\n",
    "    'Wu, Z.': 'Zhe Wu',\n",
    "    'Singh, B.': 'Bharat Singh',\n",
    "    'Davis, L.': 'Larry S. Davis',\n",
    "    'Subrahmanian, V.': 'V. S. Subrahmanian',\n",
    "}\n",
    "update_long_names(116, names)\n",
    "\n",
    "names = {\n",
    "    'Levitan, S.': 'Sarah Ita Levitan',\n",
    "    'Maredia, A.': 'Angel Maredia',\n",
    "    'Hirschberg, J.': 'Julia Hirschberg',\n",
    "}\n",
    "update_long_names(119, names)\n",
    "\n",
    "names = {\n",
    "    'Litvinova, O.': 'Olga Litvinova',\n",
    "    'Litvinova, T.': 'Tatiana Litvinova',\n",
    "    'Seredin, P.': 'Pavel Seredin',\n",
    "    'Lyell, J.': 'John Lyell',\n",
    "}\n",
    "update_long_names(130, names)\n",
    "\n",
    "names = {\n",
    "    'PÃ©rez-Rosas, V.': 'VerÃ³nica PÃ©rez-Rosas',\n",
    "    'Mihalcea, R.': 'Rada Mihalcea',\n",
    "}\n",
    "update_long_names(149, names)\n",
    "\n",
    "names = {\n",
    "    'Rubin, V.': 'Victoria L. Rubin',\n",
    "    'Conroy, N.': 'Niall J. Conroy',\n",
    "}\n",
    "update_long_names(174, names)\n",
    "\n",
    "names = {\n",
    "    'Fornaciari, T.': 'Tommaso Fornaciari',\n",
    "    'Poesio, M.': 'Massimo Poesio',\n",
    "}\n",
    "update_long_names(224, names)\n",
    "\n",
    "names = {\n",
    "    'S. V.': 'Sushma Venkatesh',\n",
    "    'R. R.': 'Raghavendra Ramachandra',\n",
    "    'P. B.': 'Patrick Bours',\n",
    "}\n",
    "update_long_names(384, names, True)\n",
    "\n",
    "names = {\n",
    "    'H. T.': 'Huawei Tao',\n",
    "    'P. L.': 'Peizhi Lei',\n",
    "    'M. W.': 'Mengzhe Wang',\n",
    "    'J. W.': 'Jie Wang',\n",
    "    'H. F.': 'Hongliang Fu',\n",
    "}\n",
    "update_long_names(385, names, True)\n",
    "\n",
    "names = {\n",
    "    'J. P.': 'Jinie Pak',\n",
    "    'L. Z.': 'Lina Zhou',\n",
    "}\n",
    "document = base.document_by({'id': 388}, return_first=True)\n",
    "update_long_names(388, names, True)\n",
    "\n",
    "names = {\n",
    "    'N. S.': 'Nidhi Srivastava',\n",
    "    'S. D.': 'Sipi Dubey',\n",
    "}\n",
    "update_long_names(389, names, True)\n",
    "\n",
    "names = {\n",
    "    'H. N.': 'Hanen Nasri',\n",
    "    'W. O.': 'Wael Ouarda',\n",
    "    'A. M. A.': 'Adel M. Alimi',\n",
    "}\n",
    "update_long_names(394, names, True)\n",
    "\n",
    "names = {\n",
    "    'J. -. Y.': 'Jun-Teng Yang',\n",
    "    'G. -. L.': 'Guei-Ming Liu',\n",
    "    'S. C. .. -. H.': 'Scott C.-H Huang',\n",
    "}\n",
    "update_long_names(395, names, True)\n",
    "\n",
    "names = {\n",
    "    'S. S.': 'Sarun Sumriddetchkajorn',\n",
    "    'A. S.': 'Armote Somboonkaew',\n",
    "    'T. S.': 'Tawee Sodsong',\n",
    "    'I. P.': 'Itthipol Promduang',\n",
    "    'N. S.': 'Niti Sumriddetchkajorn',\n",
    "    'T. P.': 'Thawatchai Prada-in',\n",
    "}\n",
    "document = base.document_by({'id': 396}, return_first=True)\n",
    "update_long_names(396, names, True)\n",
    "\n",
    "names = {\n",
    "    'U. M. S.': 'M. Umut Sen',\n",
    "    'V. P.': 'VerÃ³nica PÃ©rez-Rosas',\n",
    "    'B. Y.': 'Berrin Yanikoglu',\n",
    "    'M. A.': 'Mohamed Abouelenien',\n",
    "    'M. B.': 'Mihai Burzo',\n",
    "    'R. M.': 'Rada Mihalcea',\n",
    "}\n",
    "update_long_names(398, names, True)\n",
    "\n",
    "names = {\n",
    "    'J. O.': 'James OâShea',\n",
    "    'K. C.': 'Keeley Crockett',\n",
    "    'W. K.': 'Wasiq Khan',\n",
    "    'P. K.': 'Philippos Kindynis',\n",
    "    'A. A.': 'Athos Antoniades',\n",
    "    'G. B.': 'Georgios Boultadakis',\n",
    "}\n",
    "update_long_names(401, names, True)\n",
    "\n",
    "names = {\n",
    "    'R. R.': 'Rodrigo Rill-GarcÃ­a',\n",
    "    'H. J. E.': 'Hugo Jair Escalante',\n",
    "    'L. V.': 'Luis VillaseÃ±or-Pineda',\n",
    "    'V. R.': 'VerÃ³nica Reyes-Meza',\n",
    "}\n",
    "update_long_names(402, names, True)\n",
    "\n",
    "names = {\n",
    "    'H. K.': 'Hamid Karimi',\n",
    "    'J. T.': 'Jiliang Tang',\n",
    "    'Y. L.': 'Yanen Li',\n",
    "}\n",
    "update_long_names(403, names, True)\n",
    "\n",
    "names = {\n",
    "    'H. C.': 'Huang-Cheng Chou',\n",
    "    'Y. L.': 'Yi-Wen Liu',\n",
    "    'C. L.': 'Chi-Chun Lee',\n",
    "}\n",
    "update_long_names(410, names, True)\n",
    "\n",
    "names = {\n",
    "    'Y. X.': 'Yue Xie',\n",
    "    'R. L.': 'Ruiyu Liang',\n",
    "    'H. T.': 'Huawei Tao',\n",
    "    'Y. Z.': 'Yue Zhu',\n",
    "    'L. Z.': 'Li Zhao',\n",
    "}\n",
    "update_long_names(415, names, True)\n",
    "\n",
    "names = {\n",
    "    'C. B.': 'Chongyang Bai',\n",
    "    'M. B.': 'Maksim Bolonkin',\n",
    "    'J. B.': 'Judee D. Burgoon',\n",
    "    'C. C.': 'Chao Chen',\n",
    "    'N. D.': 'Norah Dunbar',\n",
    "    'B. S.': 'Bharat Singh',\n",
    "    'V. S. S.': 'V. S. Subrahmanian',\n",
    "    'Z. W.': 'Zhe Wu',\n",
    "}\n",
    "update_long_names(422, names, True)\n",
    "\n",
    "names = {\n",
    "    'B. A. R.': 'Bashar A. Rajoub',\n",
    "    'R. Z.': 'Reyer Zwiggelaar',\n",
    "}\n",
    "update_long_names(423, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. S.': 'Muhammad Sanaullah',\n",
    "    'K. G.': 'Kaliappan Gopalan', \n",
    "}\n",
    "update_long_names(426, names, True)\n",
    "\n",
    "names = {\n",
    "    'L. S.': 'Lin Su',\n",
    "    'M. D. L.': 'Martin D. Levine',\n",
    "}\n",
    "update_long_names(429, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. J.': 'Mimansa Jaiswal',\n",
    "    'S. T.': 'Sairam Tabibu',\n",
    "    'R. B.': 'Rajiv Bajpai',\n",
    "}\n",
    "update_long_names(432, names, True)\n",
    "\n",
    "names = {\n",
    "    'Y. A.': 'Yaniv Azar',\n",
    "    'M. C.': 'Matthew Campisi',\n",
    "}\n",
    "update_long_names(433, names, True)\n",
    "\n",
    "names = {\n",
    "    'N. H.': 'Naoki Hosomi',\n",
    "    'S. S.': 'Sakriani Sakti',\n",
    "    'K. Y.': 'Koichiro Yoshino',\n",
    "    'S. N.': 'Satoshi Nakamura',\n",
    "}\n",
    "update_long_names(434, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. A.': 'Mohamed Abouelenien',\n",
    "    'V. P.': 'VerÃ³nica PÃ©rez-Rosas',\n",
    "    'R. M.': 'Rada Mihalcea',\n",
    "    'M. B.': 'Mihai Burzo',\n",
    "}\n",
    "update_long_names(436, names, True)\n",
    "\n",
    "names = {\n",
    "    'V. G.': 'Viresh Gupta',\n",
    "    'M. A.': 'Mohit Agarwal',\n",
    "    'T. C.': 'Tanmoy Chakraborty',\n",
    "    'R. S.': 'Richa Singh',\n",
    "    'M. V.': 'Mayank Vatsa',\n",
    "}\n",
    "# Faltou o Manik Arora\n",
    "update_long_names(439, names, True)\n",
    "\n",
    "names = {\n",
    "    'X. Y.': 'Xiang Yu',\n",
    "    'S. Z.': ', Shaoting Zhang',\n",
    "    'Z. Y.': 'Zhennan Yan',\n",
    "    'F. Y.': 'Fei Yang',\n",
    "    'J. H.': 'Junzhou Huang',\n",
    "    'N. E. D.': 'Norah E. Dunbar',\n",
    "    'M. L. J.': 'Matthew L. Jensen',\n",
    "    'J. K. B.': 'Judee K. Burgoon',\n",
    "    'D. N. M.': 'Dimitris N. Metaxas',\n",
    "}\n",
    "update_long_names(448, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. D.': 'Mingyu Ding',\n",
    "    'A. Z.': 'An Zhao',\n",
    "    'Z. L.': 'Zhiwu Lu',\n",
    "    'T. X.': 'Tao Xiang',\n",
    "    'J. W.': 'Ji-Rong Wen',\n",
    "}\n",
    "update_long_names(451, names, True)\n",
    "\n",
    "names = {\n",
    "    'D. K.': 'Daniel Kopev',\n",
    "    'A. A.': 'Ahmed Ali',\n",
    "    'I. K.': 'Ivan Koychev',\n",
    "    'P. N.': 'Preslav Nakov',\n",
    "}\n",
    "update_long_names(458, names, True)\n",
    "\n",
    "names = {\n",
    "    'Y. A.': 'Yaniv Azar',\n",
    "    'M. C.': 'Matthew Campisi',\n",
    "}\n",
    "update_long_names(463, names, True)\n",
    "\n",
    "names = {\n",
    "    'H. H. T.': 'Harith H. Thannoon',\n",
    "    'W. H. A.': 'Wissam H. Ali',\n",
    "    'I. A. H.': 'Ivan A. Hashim',\n",
    "}\n",
    "update_long_names(467, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. D.': 'Malcolm Dcosta',\n",
    "    'D. S.': 'Dvijesh Shastri',\n",
    "    'R. V.': 'Ricardo Vilalta',\n",
    "    'J. K. B.': 'Judee K. Burgoon',\n",
    "    'I. P.': 'Ioannis Pavlidis',\n",
    "}\n",
    "update_long_names(483, names, True)\n",
    "\n",
    "names = {\n",
    "    'S. T.': 'Shohei Takabatake',\n",
    "    'K. S.': 'Kazutaka Shimada',\n",
    "    'T. S.': 'Takeshi Saitoh',\n",
    "}\n",
    "update_long_names(493, names, True)\n",
    "\n",
    "names = {\n",
    "    'Z. L.': 'Zuhrah Labibah',\n",
    "    'M. N.': 'Muhammad Nasrun',\n",
    "    'C. S.': 'Casi Setianingsih',\n",
    "}\n",
    "update_long_names(501, names, True)\n",
    "\n",
    "names = {\n",
    "    'E. J. B.': 'Erica J. Briscoe',\n",
    "    'D. S. A.': 'D. Scott Appling',\n",
    "    'H. H.': 'Heather Hayes',\n",
    "}\n",
    "update_long_names(504, names, True)\n",
    "\n",
    "names = {\n",
    "    'D. B.': 'Dan Barsever',\n",
    "    'S. S.': 'Sameer Singh',\n",
    "    'E. N.': 'Emre Neftci',\n",
    "}\n",
    "update_long_names(527, names, True)\n",
    "\n",
    "names = {\n",
    "    'M. R.': 'Metod Rybar',\n",
    "    'M. B.': 'Maria Bielikova',\n",
    "}\n",
    "update_long_names(546, names, True)\n",
    "\n",
    "print('\\nDONE!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
