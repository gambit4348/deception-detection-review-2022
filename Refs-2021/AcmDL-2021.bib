@article{10.1145/3448634,
author = {Zafarani, Reza and Liu, Huan and Phoha, Vir V. and Azimi, Javad},
title = {Inroduction on Recent Trends and Perspectives in Fake News Research},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2692-1626},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3448634},
doi = {10.1145/3448634},
journal = {Digital Threats},
month = {mar},
articleno = {13},
numpages = {3},
keywords = {disinformation, Fake news, fact-checking, information credibility, knowledge graph, news verification, social media, misinformation, deception detection}
}

@inproceedings{10.1145/3442442.3452328,
author = {Guo, Mingfei and Chen, Xiuying and Li, Juntao and Zhao, Dongyan and Yan, Rui},
title = {How Does Truth Evolve into Fake News? An Empirical Study of Fake News Evolution},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3442442.3452328},
doi = {10.1145/3442442.3452328},
abstract = { Automatically identifying fake news from the Internet is a challenging problem in deception detection tasks. Online news is modified constantly during its propagation, e.g., malicious users distort the original truth and make up fake news. However, the continuous evolution process would generate unprecedented fake news and cheat the original model. We present the Fake News Evolution (FNE) dataset: a new dataset tracking the fake news evolution process. Our dataset is composed of 950 paired data, each of which consists of articles representing the three significant phases of the evolution process, which are the truth, the fake news, and the evolved fake news. We observe the features during the evolution and they are the disinformation techniques, text similarity, top 10 keywords, classification accuracy, parts of speech, and sentiment properties.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {407–411},
numpages = {5},
keywords = {fake news evolution, datasets, fake news},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3500931.3500956,
author = {Duan, Yuanyuan and Shen, Xunbing},
title = {The Application of Artificial Intelligence in Emotion Detection: A Study Based on the Effect of Parenting Style on Micro-Expression Recognition Ability of College Students},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3500931.3500956},
doi = {10.1145/3500931.3500956},
abstract = {Accurate recognition of micro-expressions facilitates emotional communication, and correct capture of expressions is associated with deception detection. The combination of artificial intelligence and psychology can be widely used in various fields such as national life, agricultural production, product development, and garrison construction. This study analyzes and explores whether there is a correlation between parenting style and micro-expression recognition ability, which can provide a basis for subsequent artificial intelligence to detect emotions. This paper mainly uses offline collection of micro-expression recognition data and filling out questionnaires at the same time to collect data and conduct survey research. A total of 175 questionnaires were collected, of which 175 were valid. 110 data of micro-expression recognition were collected, of which 100 were valid. The results found that there was a significant correlation between the parenting style and micro-expression recognition ability of college students. Specifically, the rejection dimension of parents was significantly and negatively correlated with micro-expression recognition ability. Artificial intelligence has a wide range of fields, among which the combination of machine learning and expression recognition can effectively improve the possibility of screening out suspicious people in specific situations.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {137–141},
numpages = {5},
keywords = {micro-expression recognition, Artificial intelligence, parenting style},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/3442381.3450096,
author = {Wang, Yanbang and Li, Pan and Bai, Chongyang and Leskovec, Jure},
title = {TEDIC: Neural Modeling of Behavioral Patterns in Dynamic Social Interaction Networks},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3442381.3450096},
doi = {10.1145/3442381.3450096},
abstract = { Dynamic social interaction networks are an important abstraction to model time-stamped social interactions such as eye contact, speaking and listening between people. These networks typically contain informative while subtle patterns that reflect people’s social characters and relationship, and therefore attract the attentions of a lot of social scientists and computer scientists. Previous approaches on extracting those patterns primarily rely on sophisticated expert knowledge of psychology and social science, and the obtained features are often overly task-specific. More generic models based on representation learning of dynamic networks may be applied, but the unique properties of social interactions cause severe model mismatch and degenerate the quality of the obtained representations. Here we fill this gap by proposing a novel framework, termed TEmporal network-DIffusion Convolutional networks (TEDIC), for generic representation learning on dynamic social interaction networks. We make TEDIC a good fit by designing two components: 1) Adopt diffusion of node attributes over a combination of the original network and its complement to capture long-hop interactive patterns embedded in the behaviors of people making or avoiding contact; 2) Leverage temporal convolution networks with hierarchical set-pooling operation to flexibly extract patterns from different-length interactions scattered over a long time span. The design also endows TEDIC with certain self-explaining power. We evaluate TEDIC over five real datasets for four different social character prediction tasks including deception detection, dominance identification, nervousness detection and community detection. TEDIC not only consistently outperforms previous SOTA’s, but also provides two important pieces of social insight. In addition, it exhibits favorable societal characteristics by remaining unbiased to people from different regions. Our project website is: http://snap.stanford.edu/tedic/.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {693–705},
numpages = {13},
keywords = {representation learning, social network dynamics, social interactions},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inbook{10.1145/3474085.3478579,
author = {Li, Jingting and Yap, Moi Hoon and Cheng, Wen-Huang and See, John and Hong, Xiaopeng and Li, Xiaobai and Wang, Su-Jing},
title = {FME'21: 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3474085.3478579},
abstract = {Facial micro-expressions (FMEs) are involuntary facial movements that occur spontaneously when a person experiences an emotion but tries to suppress or repress the facial expression and usually occur in high-risk situations. Thus, FMEs are very short in duration, an important feature that distinguishes them from ordinary facial expressions. And MEs are considered to be one of the most valuable cues for complex human emotion understanding and lie detection. Since 2014, the computational analysis and automation of MEs have been an emerging area of face research. The workshop will explore various dimensions of the human mind through emotion understanding and FME analysis, as well as extended research based on multi modal approaches.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5700–5701},
numpages = {2}
}

@inproceedings{10.1145/3447587.3447596,
author = {Li, Mengya and Chen, Lei and Wei, Wenhui and Ben, Xianye and Wang, Deqiang},
title = {An Improved Generative Adversarial Network for Micro-Expressions Based on Multi-Label Learning from Action Units},
year = {2021},
isbn = {9781450389105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3447587.3447596},
doi = {10.1145/3447587.3447596},
abstract = {Micro-expression is a spontaneous facial expression with short duration, low intensity and facial partial action units. Micro-expression recognition plays an important role in psychological diagnosis, lie detection, and security systems. However, even the state-of-the-art recognition models suffer from the lack of micro-expression samples. In order to augment the training data, we propose a new method — an improved Generative adversarial network (GAN) for micro-expressions based on multi-label learning from action units. In the proposed model, action units (AUs) are added to GAN in the form of multi-labels. The designed loss function ensures to generate high quality images. The designed loss function for video can obtain a smooth action video with a trajectory of the AU. The designed loss function of optical flow guarantees low intensity for micro-expression generation and high similarity with the original training micro-expression samples. Moreover, the micro-expression recognition accuracy can be improved via adding the generated samples to the training data set. The experimental results on the two benchmark databases including CASME II and MMEW demonstrate the superiority of the proposed method over other state-of-the-art micro-expression recognition methods.},
booktitle = {2021 The 4th International Conference on Image and Graphics Processing},
pages = {59–64},
numpages = {6},
keywords = {Action unit (AU), Micro-expression recognition, Generative adversarial network (GAN), Multi-label learning},
location = {Sanya, China},
series = {ICIGP 2021}
}

@inproceedings{10.1145/3476100.3484463,
author = {Pan, Hang and Xie, Lun and Wang, Zhiliang},
title = {Spatio-Temporal Convolutional Attention Network for Spotting Macro- and Micro-Expression Intervals},
year = {2021},
isbn = {9781450386838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3476100.3484463},
doi = {10.1145/3476100.3484463},
abstract = {Emotional detection based on facial expressions is an important procedure in high-risk tasks such as criminal investigation or lie detection. To reduce the impact of the inconsistency in the duration of macro- and micro-expression, we propose an effective Spatio-temporal Convolutional Attention Network (STCAN) for spotting macro- and micro-expression intervals in long video sequences. The spatial features of each image in the video sequence are extracted through the Convolution Neural Network. Then, considering the problem of the inconsistency in the duration of the macro- and micro-expression, the multi-head self-attention model is used to analyze the weight of the spatial feature of the image in the temporal space. Finally, the time interval of emotional changes is determined according to the weight of each frame of the video sequence, and the macro- and micro-expression intervals are obtained through the threshold segmentation model. Considering the problem of Leave-One-Subject-Out cross-validation the training time long, we verified the effectiveness of our model on the SAMM Long Video and CAS(ME)2 datasets through the Leave-Half-Subject-Out (LHSO) cross-validation method. The experiments show that the STCAN model can achieve competitive results on Facial Micro-Expression (FME) Challenge 2021.},
booktitle = {Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting},
pages = {25–30},
numpages = {6},
keywords = {spatio-temporal feature, attention model, micro-expression, macro-expression, convolution},
location = {Virtual Event, China},
series = {FME'21}
}

@article{10.1145/3458791,
author = {Edla, Damodar Reddy and Dodia, Shubham and Bablani, Annushree and Kuppili, Venkatanareshbabu},
title = {An Efficient Deep Learning Paradigm for Deceit Identification Test on EEG Signals},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2158-656X},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3458791},
doi = {10.1145/3458791},
abstract = {Brain-Computer Interface is the collaboration of the human brain and a device that controls the actions of a human using brain signals. Applications of brain-computer interface vary from the field of entertainment to medical. In this article, a novel Deceit Identification Test is proposed based on the Electroencephalogram signals to identify and analyze the human behavior. Deceit identification test is based on P300 signals, which have a positive peak from 300 ms to 1,000 ms of the stimulus onset. The aim of the experiment is to identify and classify P300 signals with good classification accuracy. For preprocessing, a band-pass filter is used to eliminate the artifacts. The feature extraction is carried out using “symlet” Wavelet Packet Transform (WPT). Deep Neural Network (DNN) with two autoencoders having 10 hidden layers each is applied as the classifier. A novel experiment is conducted for the collection of EEG data from the subjects. EEG signals of 30 subjects (15 guilty and 15 innocent) are recorded and analyzed during the experiment. BrainVision recorder and analyzer are used for recording and analyzing EEG signals. The model is trained for 90% of the dataset and tested for 10% of the dataset and accuracy of 95% is obtained.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jun},
articleno = {25},
numpages = {20},
keywords = {Brain-computer interface, deep neural network, deceit identification test, electroencephalogram, wavelet packet transform}
}

@inproceedings{10.1145/3482632.3487459,
author = {Li, Chunxu},
title = {Electronic Countermeasure Technology of Supporting Ship Intelligent Navigation},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3482632.3487459},
doi = {10.1145/3482632.3487459},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2504–2508},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3503181.3503220,
author = {Luo, Yiqin and Zhang, Tongda and Sun, Xiao and Chen, Minyu},
title = {Collaboration Interaction Prediction in Crowdsourcing via Temporal Network},
year = {2021},
isbn = {9781450395540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3503181.3503220},
doi = {10.1145/3503181.3503220},
abstract = { Networks are widely used as a popular representation for describing complex systems of interacting entities, especially in the domain of Crowd Intelligent Science and Engineering. However, most research has focused on static snapshots of the Networks, which has largely ignored the temporal dynamics of the network. Also, it is usually challenging to predict cooperative behavior between independent contractors via a static network. The contribution of this paper is two-fold. First, we provide a crowd collaboration incentive system under full pay transparency on a temporal scale. Second, we predict a strong cooperation relationship between independent contractors via inductive presentation learning of temporal networks. Our findings suggest that 1) strong cooperation relationships exist widely in collaboration networks, which can help distribution, efficiency incentive, and workload intervention, and 2) strong cooperation relationships can be operationalized and predicted via computational methods.},
booktitle = {5th International Conference on Crowd Science and Engineering},
pages = {8–11},
numpages = {4},
keywords = {graph learning, network embedding, temporal networks, collective intelligence},
location = {Jinan, China},
series = {ICCSE '21}
}

@inproceedings{10.1145/3453800.3453821,
author = {Thi Thu Nguyen, Nhi and Thi Thu Nguyen, Duyen and The Pham, Bao},
title = {Micro-Expression Recognition Based on the Fusion between Optical Flow and Dynamic Image},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3453800.3453821},
doi = {10.1145/3453800.3453821},
abstract = {Micro-expression (ME) is a subtle and involuntary facial expression that reveals the human's concealed emotion. Psychology studies pointed out that research of ME can build potential applications in many fields. Therefore, ME recognition (MER), one of the two main ME analysis tasks, has been becoming an attractive topic recently. However, the work of MER is still needed to consider due to several limitations related to performance and dataset. This paper proposes a feature fusion between optical flow and dynamic image to create a robust ME representation, which can be learned effectively from deep learning techniques. Experiments from two public datasets, CASME-II and SAMM, show that our method obtains higher performance than several existing studies and is very promising for future research.CCS CONCEPTS •Computing methodologies∼Object recognition},
booktitle = {2021 The 5th International Conference on Machine Learning and Soft Computing},
pages = {115–120},
numpages = {6},
keywords = {Dynamic Image, Optical Flow, Affective Computing, Micro-expression},
location = {Da Nang, Viet Nam},
series = {ICMLSC'21}
}

@inproceedings{10.1145/3397481.3450650,
author = {Wang, Xinru and Yin, Ming},
title = {Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3397481.3450650},
doi = {10.1145/3397481.3450650},
abstract = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making. },
booktitle = {26th International Conference on Intelligent User Interfaces},
pages = {318–328},
numpages = {11},
keywords = {trust calibration, human-subject experiments, explainable AI, interpretable machine learning, trust},
location = {College Station, TX, USA},
series = {IUI '21}
}

@article{10.1145/3479552,
author = {Liu, Han and Lai, Vivian and Tan, Chenhao},
title = {Understanding the Effect of Out-of-Distribution Examples and Interactive Explanations on Human-AI Decision Making},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3479552},
doi = {10.1145/3479552},
abstract = {Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {408},
numpages = {45},
keywords = {interactive explanations, appropriate trust, human-AI decision making, complementary performance, distribution shift}
}

@inproceedings{10.1145/3508546.3508581,
author = {Wang, Yu and Xu, XinMin and Zhuang, Yao},
title = {Learning Dynamics for Video Facial Expression Recognition},
year = {2021},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3508546.3508581},
doi = {10.1145/3508546.3508581},
abstract = { Video-based facial expression recognition has always been a focus of attention in the computer vision community for decades. It aims to automatically identify one of several emotions represented by the video according to the input audio or visual information. Capturing the dynamics, namely motion pattern plays an important role in video-based facial expression recognition. In this paper, we explore an effective and efficient motion pattern to model temporal relationships called Diff-based Canny Operator (DCO) to guide the inter-frame aggregation and generate a novel feature modality. Our proposed DCO add rare computational consumption and can be easily inserted into any frameworks, so we incorporate it with exist networks to form a unified structure for video-based facial expression recognition task, which enable the network to Ideally extract temporal information. With extensive experiments on CK+ and AFEW dataset, our proposed method shows its superiority with better or comparable performance compared to the state-of-the-art approaches at low FLOPs.},
booktitle = {2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {35},
numpages = {5},
keywords = {dynamics, Diff-based Canny Operator, video-based facial expression recognition, motion pattern},
location = {Sanya, China},
series = {ACAI'21}
}

@inproceedings{10.1145/3478905.3478935,
author = {Li, Guangjie and Ma, Jiajun and Zhan, Qiang and Wu, Dandan and Wei, Xiong and Wang, Xu and Zuo, Jiancun},
title = {Detection Technology of Satellite Navigation Spoofing Signal},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3478905.3478935},
doi = {10.1145/3478905.3478935},
abstract = {In the working process of the satellite navigation system, there are certain risks of security, among which spoofing attack is the main reason for threatening information security. Therefore, domestic and foreign scholars have conducted numerous studies on the spoofing signal detection technology of GNSS (Global Navigation Satellite System). In this paper, the spoofing principle of satellite navigation is introduced, the common spoofing detection technologies are discussed in-depth, and the performance of existing technologies is compared and analyzed. Finally, the future research directions are summarized and prospected.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {148–153},
numpages = {6},
keywords = {spoofing principle, spoofing signal, Keywords: satellite navigation, spoofing detection technologies},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3453892.3461330,
author = {Clark, Addison and Ahmad, Ishfaq},
title = {Interfacing with Robots without the Use of Touch or Speech},
year = {2021},
isbn = {9781450387927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3453892.3461330},
doi = {10.1145/3453892.3461330},
abstract = { As the field of robotics develops, so do the methods of interfacing with and controlling those robots. Many modern robots can be communicated with using commands that require neither speech nor touch. The two main motivations behind this trend are the assistance of people with disabilities and the desire for more natural Human-Robot Interaction (HRI). In allowing nonverbal communication with robots, accessibility of the systems increases to allow more people to interact with and benefit from robotics systems. Additionally, nonverbal communication provides more natural communication, which can lead to many benefits in HRI. This paper provides an overview of existing technologies in touchless and nonverbal human-robot interfaces; it presents the most prevalent interface methods, their uses, and limitations. Discussions are provided on future directions for research.},
booktitle = {The 14th PErvasive Technologies Related to Assistive Environments Conference},
pages = {347–353},
numpages = {7},
keywords = {emotion detection, EEG, gaze detection, HRI},
location = {Corfu, Greece},
series = {PETRA 2021}
}

@inbook{10.1145/3474085.3475674,
author = {Yan, Jingwei and Wang, Jingjing and Li, Qiang and Wang, Chunmao and Pu, Shiliang},
title = {Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3474085.3475674},
abstract = {Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1038–1046},
numpages = {9}
}

@inproceedings{10.1145/3397481.3450698,
author = {Desmond, Michael and Muller, Michael and Ashktorab, Zahra and Dugan, Casey and Duesterwald, Evelyn and Brimijoin, Kristina and Finegan-Dollak, Catherine and Brachman, Michelle and Sharma, Aabhas and Joshi, Narendra Nath and Pan, Qian},
title = {Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3397481.3450698},
doi = {10.1145/3397481.3450698},
abstract = {Labeling data is an important step in the supervised machine learning lifecycle. It is a laborious human activity comprised of repeated decision making: the human labeler decides which of several potential labels to apply to each example. Prior work has shown that providing AI assistance can improve the accuracy of binary decision tasks. However, the role of AI assistance in more complex data-labeling scenarios with a larger set of labels has not yet been explored. We designed an AI labeling assistant that uses a semi-supervised learning algorithm to predict the most probable labels for each example. We leverage these predictions to provide assistance in two ways: (i) providing a label recommendation and (ii) reducing the labeler’s decision space by focusing their attention on only the most probable labels. We conducted a user study (n=54) to evaluate an AI-assisted interface for data labeling in this context. Our results highlight that the AI assistance improves both labeler accuracy and speed, especially when the labeler finds the correct label in the reduced label space. We discuss findings related to the presentation of AI assistance and design implications for intelligent labeling interfaces. },
booktitle = {26th International Conference on Intelligent User Interfaces},
pages = {392–401},
numpages = {10},
keywords = {Human Computer Interaction., Data Labeling, AI Assistance},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1145/3462244.3482855,
author = {Oviatt, Sharon},
title = {Technology as Infrastructure for Dehumanization: Three Hundred Million People with the Same Face},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3462244.3482855},
doi = {10.1145/3462244.3482855},
abstract = {Based on humanistic psychology, this paper reflects on how and why technology has led to increasing dehumanization within society. The literature on human autonomy, or individuals’ ability to exercise agency and control over their own lives, emphasizes that erosion of autonomy adversely impacts human behavior and health—including demotivating people, heightening their anxiety and apathy, undermining interpersonal relations, reducing self-efficacy and learning, and damaging mental and physical health in substantial ways. This paper describes concrete examples of how technology is accelerating loss of human autonomy, which often occurs during invasive surveillance and covert manipulation during user-technology interactions. These examples highlight abuses of multimodal-multisensor technologies, especially during education. An analysis is provided of the psychosocial context that encourages people to use technology in abusive ways, which directly fuels the growing schism between those who control and those who are disempowered victims. Five specific directions are outlined for how our community can design future technology using counter tactics, practices, and policies that strengthen rather than eroding autonomy and well-being.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {278–287},
numpages = {10},
keywords = {privacy, surveillance, human autonomy, school children, ethics, abuse of technology, anxiety, fragile elderly, Human-centered A.I., multimodal-multisensor technology, societal impact, health, dehumanization},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3411763.3451721,
author = {Song, Byung Cheol and Kim, Dae Ha},
title = {Hidden Emotion Detection Using Multi-Modal Signals},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3411763.3451721},
doi = {10.1145/3411763.3451721},
abstract = { In order to better understand human emotion, we should not recognize only superficial emotions based on facial images, but also analyze so-called inner emotions by considering biological signals such as electroencephalogram (EEG). Recently, several studies to analyze a person’s inner state by using an image signal and an EEG signal together have been reported. However, there have been no studies dealing with the case where the emotions estimated from the image signal and the EEG signal are different, i.e., emotional mismatch. This paper defines a new task to detect hidden emotions, i.e., emotions in a situation where only the EEG signal is activated without the image signal being activated, and proposes a method to effectively detect the hidden emotions. First, when a subject hides the emotion intentionally, the internal and external emotional characteristics of the subject were analyzed from the viewpoint of multimodal signals. Then, based on the analysis, we designed a method of detecting hidden emotions using convolutional neural networks (CNNs) that exhibit powerful cognitive ability. As a result, this study has upgraded the technology of deeply understanding inner emotions. On the other hand, the hidden emotion dataset and source code that we have built ourselves will be officially released for future emotion recognition research.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {413},
numpages = {7},
keywords = {Convolutional Neural Networks, Hidden emotion detection, inner emotion},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@article{10.1109/TASLP.2020.3036611,
author = {Cheng, Jiaming and Liang, Ruiyu and Liang, Zhenlin and Zhao, Li and Huang, Chengwei and Schuller, Bj\"{o}rn},
title = {A Deep Adaptation Network for Speech Enhancement: Combining a Relativistic Discriminator With Multi-Kernel Maximum Mean Discrepancy},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1109/TASLP.2020.3036611},
doi = {10.1109/TASLP.2020.3036611},
abstract = {In deep-learning-based speech enhancement (SE) systems, trained models are often used to handle unseen noise types and language environments in real-life scenarios. However, since production environments differ from training conditions, mismatch problems arise that may cause a serious decrease in the performance of an SE system. In this study, a domain adaptive method combining two adaptation strategies is proposed to improve the generalization of unlabeled noisy speech. In the proposed encoder-decoder-based SE framework, a domain discriminator and a domain confusion adaptation layer are introduced to conduct adversarial training. The model has two main innovations. First, the algorithm optimizes adversarial training by introducing a relativistic discriminator that relies on relative values by applying the difference, thus avoiding possible bias and better reflecting domain differences. Second, the multi-kernel maximum mean discrepancy (MK-MMD) between domains is taken as the regularization term of the domain adversarial loss, thereby further decreasing the edge distribution distance between domains. The proposed model improves the adaptability to unseen noises by encouraging the feature encoder to generate domain-invariant features. The model was evaluated using cross-noise and cross-language-and-noise experiments, and the results show that the proposed method provides considerable improvements over the baseline without an adaptation in the perceptual evaluation of speech quality (PESQ), the short time objective intelligibility (STOI) and the frequency-weighted signal-to-noise ratio (FWSNR).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {41–53},
numpages = {13}
}

@inproceedings{10.1145/3411764.3445717,
author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
title = {Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3411764.3445717},
doi = {10.1145/3411764.3445717},
abstract = { Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {81},
numpages = {16},
keywords = {Human-AI teams, Explainable AI, Augmented intelligence},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445118,
author = {Warner, Mark and Lascau, Laura and Cox, Anna L and Brumby, Duncan P and Blandford, Ann},
title = {“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3411764.3445118},
doi = {10.1145/3411764.3445118},
abstract = { Message deletion in mobile messaging apps allows people to “unsay” things they have said. This paper explores how and why people use (or do not use) this feature within remediation strategies after a communication error is identified. We present findings from a multi-stage survey designed to explore people’s general experiences of the message deletion feature (N = 401), peoples’ experiences of using this feature during the remediation of an error (N = 70), and receivers’ perceptions around recent message deletions (N = 68). While people are typically aware of the deletion feature, it is infrequently used. When used, it is primarily done so to improve conversations by reducing confusion between conversation partners. We found people being aware of message deletions creating information-gaps which can provoke curiosity in recipients, causing them to develop narratives to help address the uncertainty. We found concerns amongst senders that these narratives would be of a negative nature, having an undesirable impact on how others perceive them. We use our findings to suggest ways in which mobile messaging apps could improve conversational experiences around erroneous and regrettable messages. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {343},
numpages = {13},
keywords = {messaging errors, delete notification, unsent messages, message deletion, errors in communication, mobile messaging, delete indicator, messaging apps, global delete, regrettable messages},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3434073.3444682,
author = {Pasquali, Dario and Gonzalez-Billandon, Jonas and Rea, Francesco and Sandini, Giulio and Sciutti, Alessandra},
title = {Magic ICub: A Humanoid Robot Autonomously Catching Your Lies in a Card Game},
year = {2021},
isbn = {9781450382892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3434073.3444682},
doi = {10.1145/3434073.3444682},
abstract = {Games are often used to foster human partners' engagement and natural behavior, even when they are played with or against robots. Therefore, beyond their entertainment value, games represent ideal interaction paradigms where to investigate natural human-robot interaction and to foster robots' diffusion in the society. However, most of the state-of-the-art games involving robots, are driven with a Wizard of Oz approach. To address this limitation, we present an end-to-end (E2E) architecture to enable the iCub robotic platform to autonomously lead an entertaining magic card trick with human partners. We demonstrate that with this architecture a robot is capable of autonomously directing the game from beginning to end. In particular, the robot could detect in real-time when the players lied in the description of one card in their hands (the secret card). In a validation experiment the robot achieved an accuracy of 88.2% (against a chance level of 16.6%) in detecting the secret card while the social interaction naturally unfolded. The results demonstrate the feasibility of our approach and its effectiveness in entertaining the players and maintaining their engagement. Additionally, we provide evidence on the possibility to detect important measures of the human partner`s inner state such as cognitive load related to lie creation with pupillometry in a short and ecological game-like interaction with a robot.},
booktitle = {Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {293–302},
numpages = {10},
keywords = {human-robot interaction, entertainment, cognitive load, magic, pupillometry},
location = {Boulder, CO, USA},
series = {HRI '21}
}

@inproceedings{10.1145/3488933.3488994,
author = {Wang, Shuyan and Wei, Wei and Zhang, Meng},
title = {Sentiment Analysis Model Based on Multi-Head Attention in Multimodality},
year = {2021},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3488933.3488994},
doi = {10.1145/3488933.3488994},
abstract = {To solve the problem that different in emotion analysis have different degrees of influence on the target discourse, which leads to the unsatisfactory effect of emotion classification, This paper proposes a sentiment analysis model based on multi-head attention mechanism. Firstly, the text, visual and acoustic features are extracted from the preprocessed utterance information; Then, the multi-head attention mechanism is used to label the weight of the data, and the data information of different positions is extracted from multiple subspaces to get high-level semantic features; Next, The data fusion of context-independent feature extraction is input into LSTM network to generate context-dependent feature extraction; finally, the data is input to the softmax layer to get the result of sentiment classification. The experimental results show that compared to other existing models on the CMU-MOSI data set and IEMOCAP data set, it is increased respectively to 84.48% and 79.61%. Therefore, the model in this paper has a better sentiment analysis effect.},
booktitle = {2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {587–591},
numpages = {5},
location = {Xiamen, China},
series = {AIPR 2021}
}

@inbook{10.1145/3474085.3479209,
author = {Yang, Bo and Wu, Jianming and Zhou, Zhiguang and Komiya, Megumi and Kishimoto, Koki and Xu, Jianfeng and Nonaka, Keisuke and Horiuchi, Toshiharu and Komorita, Satoshi and Hattori, Gen and Naito, Sei and Takishima, Yasuhiro},
title = {Facial Action Unit-Based Deep Learning Framework for Spotting Macro- and Micro-Expressions in Long Video Sequences},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3474085.3479209},
abstract = {In this paper, we utilize facial action units (AUs) detection to construct an end-to-end deep learning framework for the macro- and micro-expressions spotting task in long video sequences. The proposed framework focuses on individual components of facial muscle movement rather than processing the whole image, which eliminates the influence of image change caused by noises, such as body or head movement. Compared with existing models deploying deep learning methods with classical Convolutional Neural Network (CNN) models, the proposed framework utilizes Gated Recurrent Unit (GRU) or Long Short-term Memory (LSTM) or our proposed Concat-CNN models to learn the characteristic correlation between AUs of distinctive frames. The Concat-CNN uses three convolutional kernels with different sizes to observe features of different duration and emphasizes both local and global mutation features by changing dimensionality (max-pooling size) of the output space. Our proposal achieves state-of-the-art performance from the aspect of overall F1-scores: 0.2019 on CAS(ME)2-cropped, 0.2736 on SAMM Long Video, and 0.2118 on CAS(ME)2, which not only outperforms the baseline but is also ranked the 3rd of FME challenge 2021 for combined datasets of CAS(ME)2-cropped and SAMM-LV.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4794–4798},
numpages = {5}
}

@article{10.1613/jair.1.12814,
author = {Cheng, Lu and Varshney, Kush R. and Liu, Huan},
title = {Socially Responsible AI Algorithms: Issues, Purposes, and Challenges},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1613/jair.1.12814},
doi = {10.1613/jair.1.12814},
abstract = {In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI’s indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.
This article appears in the special track on AI &amp; Society.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {1137–1181},
numpages = {45},
keywords = {machine learning, neural networks, data mining, causality}
}

@article{10.1145/3427376,
author = {Bernal, Sergio L\'{o}pez and Celdr\'{a}n, Alberto Huertas and P\'{e}rez, Gregorio Mart\'{\i}nez and Barros, Michael Taynnan and Balasubramaniam, Sasitharan},
title = {Security in Brain-Computer Interfaces: State-of-the-Art, Opportunities, and Future Challenges},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3427376},
doi = {10.1145/3427376},
abstract = {Brain-Computer Interfaces (BCIs) have significantly improved the patients’ quality of life by restoring damaged hearing, sight, and movement capabilities. After evolving their application scenarios, the current trend of BCI is to enable new innovative brain-to-brain and brain-to-the-Internet communication paradigms. This technological advancement generates opportunities for attackers, since users’ personal information and physical integrity could be under tremendous risk. This work presents the existing versions of the BCI life-cycle and homogenizes them in a new approach that overcomes current limitations. After that, we offer a qualitative characterization of the security attacks affecting each phase of the BCI cycle to analyze their impacts and countermeasures documented in the literature. Finally, we reflect on lessons learned, highlighting research trends and future challenges concerning security on BCIs.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {11},
numpages = {35},
keywords = {BCI, Brain-computer interfaces, cybersecurity, privacy, safety}
}

@article{10.1145/3470742,
author = {Diel, Alexander and Weigelt, Sarah and Macdorman, Karl F.},
title = {A Meta-Analysis of the Uncanny Valley's Independent and Dependent Variables},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
url = {https://doi-org.ez22.periodicos.capes.gov.br/10.1145/3470742},
doi = {10.1145/3470742},
abstract = {The uncanny valley (UV) effect is a negative affective reaction to human-looking artificial entities. It hinders comfortable, trust-based interactions with android robots and virtual characters. Despite extensive research, a consensus has not formed on its theoretical basis or methodologies. We conducted a meta-analysis to assess operationalizations of human likeness (independent variable) and the UV effect (dependent variable). Of 468 studies, 72 met the inclusion criteria. These studies employed 10 different stimulus creation techniques, 39 affect measures, and 14 indirect measures. Based on 247 effect sizes, a three-level meta-analysis model revealed the UV effect had a large effect size, Hedges’ g = 1.01 [0.80, 1.22]. A mixed-effects meta-regression model with creation technique as the moderator variable revealed face distortion produced the largest effect size, g = 1.46 [0.69, 2.24], followed by distinct entities, g = 1.20 [1.02, 1.38], realism render, g = 0.99 [0.62, 1.36], and morphing, g = 0.94 [0.64, 1.24]. Affective indices producing the largest effects were threatening, likable, aesthetics, familiarity, and eeriness, and indirect measures were dislike frequency, categorization reaction time, like frequency, avoidance, and viewing duration. This meta-analysis—the first on the UV effect—provides a methodological foundation and design principles for future research.},
journal = {J. Hum.-Robot Interact.},
month = {oct},
articleno = {1},
numpages = {33},
keywords = {computer animation, robotics, Anthropomorphism, face perception, uncanny valley}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

